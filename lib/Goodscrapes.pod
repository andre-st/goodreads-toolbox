=pod

=encoding utf8

=head1 NAME

Goodscrapes - Simple Goodreads.com scraping helpers


=head1 VERSION

=over

=item * Updated: 2018-07-21

=item * Since: 2014-11-05

=back

=head1 COMPARED TO THE OFFICIAL API

=over

=item * focuses on analysing, not updating info on GR

=item * less limited, e.g., reading shelves and reviews of other members

=item * official API is slow too; API users are even second-class citizen

=item * theoretically this library is more likely to break, 
        but Goodreads progresses very very slowly: nothing
        actually broke since 2014 (I started this);
        actually their API seems to change more often than
        their web pages; they can and do disable API functions 
        without being noticed by the majority, but they cannot
        easily disable important webpages that we use too

=item * this library grew with every new usecase and software;
        it retries operations on errors on Goodreads.com,
        which are not seldom (over capacity, exceptions etc);
        it saw a lot of flawed data such as wrong review dates 
        ("Jan 01, 1010"), which broke Time::Piece.

=item * Goodreads "aren't eating their own dog food"
        https://www.goodreads.com/topic/show/18536888-is-the-public-api-maintained-at-all#comment_number_1

=back


=head1 LIMITATIONS

=over

=item * slow: version with concurrent AnyEvent::HTTP requests was marginally 
        faster, so I sticked with simpler code; doesn't actually matter
        due to Amazon's and Goodreads' request throttling. You can only
        speed things up significantly with a pool of work-sharing computers 
        and unique IP addresses...

=item * just text pattern matching, no ECMAScript execution and DOM parsing
        (so far sufficient and faster)

=back


=head1 AUTHOR

https://github.com/andre-st/


=head1 DATA STRUCTURES

=head2 Note

=over

=item * never cast 'id' to int or use %d format string, despite digits only, 
        compare as strings

=item * don't expect all attributes set (C<undef>), this depends on context

=back


=head2 %book

=over

=item * id          => C<string>

=item * title       => C<string>

=item * isbn        => C<string>

=item * num_ratings => C<int>

=item * user_rating => C<int>

=item * url         => C<string>

=item * img_url     => C<string>

=item * author      => C<L<%user|"%user">>

=back


=head2 %user

=over

=item * id         => C<string>

=item * name       => C<string>

=item * age        => C<int> (not supported yet)

=item * is_friend  => C<bool>

=item * is_author  => C<bool>

=item * is_female  => C<bool> (not supported yet)

=item * is_private => C<bool> (not supported yet)

=item * is_staff   => C<bool> (not supported yet), is a Goodreads.com employee

=item * url        => C<string> URL to the user's profile page

=item * works_url  => C<string> URL to the author's distinct works (is_author == 1)

=item * img_url    => C<string>

=back


=head2 %review

=over

=item * id          => C<string>

=item * user        => C<L<%user|"%user">>

=item * book_id     => C<string>

=item * rating      => C<int> 
                       with 0 meaning no rating, "added" or "marked it as abandoned" 
                       or something similar

=item * rating_str  => C<string> 
                       represention of rating, e.g., 3/5 as S<"[***  ]"> or S<"[TTT  ]"> 
                       if there's additional text

=item * text        => C<string>

=item * date        => C<Time::Piece>

=item * review_url  => C<string>

=back

=head1 PUBLIC SUBROUTINES



=head2 C<string> require_good_userid( I<$user_id_to_verify> )

=over

=item * returns a sanitized, valid Goodreads user id or kills 
        the current process with an error message

=back

=head2 C<string> require_good_shelfname( I<$name_to_verify> )

=over

=item * returns the given shelf name if valid 

=item * returns a shelf which includes all books if no name given

=item * kills the current process with an error message if name is malformed

=back

=head2 C<bool> is_bad_profile( I<$user_or_author_id> )

=over

=item * returns true if blacklisted users or author who dirties and slows down any analysis

=item * "NOT A BOOK" author (3.000+ books), "Anonymous" author (10.000 books),
        non-orgs with 100.000+ books (probably bots or analytics accounts) etc

=back

=head2 C<void> set_good_cookie( I<$cookie_content_str> )

=over

=item * some Goodreads.com pages are only accessible by authenticated members

=item * copy-paste cookie from Chrome's DevTools network-view

=back

=head2 C<void> set_good_cookie_file( I<$path_to_cookie_file = '.cookie'> )

=head2 C<bool> test_good_cookie()

=over

=item * not supported at the moment

=back

=head2 C<void> set_good_cache( I<$number, $unit = 'days'> )

=over

=item * scraping Goodreads.com is a very slow process

=item * scraped documents can be cached if you don't need them "fresh"

=item * e.g., during development time

=item * e.g., during long running sessions (cheap recovery on crash, power blackout or pauses)

=item * e.g., when experimenting with parameters

=item * unit can be C<"minutes">, C<"hours">, C<"days">

=back

=head2 C<(L<%book|"%book">,...)> query_good_books( I<$user_id, $shelf_name> )

=head2 C<int> query_good_author_books( I<$books_array_ref, $author_id> )

=over

=item * I<$books_array_ref>: C<(L<%book|"%book">,...)>

=item * returns the number of books of the given author

=back

=head2 C<(L<%review|"%review">,...)> query_good_reviews(I<{ 
	book =E<gt> C<L<%book|"%book">>, since =E<gt>  undef, stalltime =E<gt> undef, 
	on_progress =E<gt> undef, use_dict = 1 }>)

=over

=item * loads ratings (no text), reviews (text), "to-read", "added" etc;
        you can filter yourself afterwards

=item * optional I<since> argument of type C<Time::Piece>

=item * optional I<use_dict>: try to find additional reviews by using the 
        text-search function provided by Goodreads.com

=item * optional I<stalltime> is the number of seconds to wait for 
        new reviews when trying to find additional reviews, 
        aborts if exceeded

=item * if I<stalltime> is set to 0 (fastest), then the latest
        reviews only are considered (max. 300 reviews)

=item * set I<stalltime> to a very large value if you want the search take 
        as long as it needs, which is okay for a project on a single book,
        but would take too long for 1000 books

=item * optional I<on_progress> callback function is called with an
        argument, which contains the number of currently loaded reviews
        (use %5s in format strings)

=back

=head2 C<(id =E<gt> L<%user|"%user">,...)> query_good_followees( I<$user_id> )

=over

=item * Precondition: set_good_cookie()

=item * returns friends AND followees

=back

=head2 C<(L<%user|"%user">,...)> query_similar_authors( I<$author_id> )

=head2 C<string> amz_book_html( I<L<%book|"%book">> )

=over

=item * HTML body of an Amazon article page

=back

=head1 PRIVATE SUBROUTINES



=head2 C<string> _amz_url( I<L<%book|"%book">> )

=over

=item * Requires at least {isbn=>string}

=back

=head2 C<string> _shelf_url( I<$user_id, $shelf_name, $page_number = 1> )

=over

=item * URL for a page with a list of books (not all books)

=item * "&per_page=100" has no effect (GR actually loads 5x 20 books via JavaScript)

=item * "&print=true" not included, any advantages?

=item * "&view=table" puts I<all> book data in code, although invisible (display=none)

=item * "&sort=rating" is important for `friendrated.pl` with its book limit:
        Some users read 9000+ books and scraping would take forever. 
        We sort lower-rated books to the end and just scrape the first pages:
        Even those with 9000+ books haven't top-rated more than 2700 books.

=item * B<Warning:> changes to the URL structure will bust the file-cache

=back

=head2 C<string> _followees_url( I<$user_id, $page_number = 1> )

=over

=item * URL for a page with a list of the people $user is following

=item * B<Warning:> changes to the URL structure will bust the file-cache

=back

=head2 C<string> _friends_url( I<$user_id, $page_number = 1> )

=over

=item * URL for a page with a list of people befriended to C<$user_id>

=item * "&sort=date_added" (as opposed to 'last online') avoids 
        moving targets while reading page by page

=item * "&skip_mutual_friends=false" because we're not doing
        this just for me

=item * B<Warning:> changes to the URL structure will bust the file-cache

=back

=head2 C<string> _book_url( I<$book_id> )

=head2 C<string> _user_url( I<$user_id, $is_author = 0> )

=head2 C<string> _revs_url( I<$book_id, $str_sort_newest_oldest = undef, 
		$search_text = undef, $rating = undef, $page_number = 1> )

=over

=item * "&sort=newest" and "&sort=oldest" reduce the number of reviews for 
        some reason (also observable on the Goodreads website), 
        so only use if really needed (&sort=default)

=item * "&search_text=example", max 30 hits, invalidates sort order argument

=item * "&rating=5"

=item * the maximum of retrievable pages is 10 (300 reviews)

=item * seems less throttled, not true for text-search

=back

=head2 C<string> _rev_url( I<$review_id> )

=head2 C<string> _author_books_url( I<$user_id, $page_number = 1> )

=head2 C<string> _author_followings_url( I<$author_id, $page_number = 1> )

=head2 C<string> _similar_authors_url( I<$author_id> )

=over

=item * page number > N just returns same page, so no easy stop criteria;
        not sure, if there's more than page, though

=back

=head2 C<bool> _extract_books( I<$result_array_ref, $shelf_tableview_html_str> )

=over

=item * I<$result_array_ref>: C<(L<%book|"%book">,...)>

=back

=head2 C<bool> _extract_author_books( I<$result_array_ref, $html_str> )

=over

=item * I<$result_array_ref>: C<(L<%book|"%book">,...)> 

=back

=head2 C<bool> _extract_followees( I<$result_hash_ref, $following_page_html_str> )

=over

=item * I<$result_hash_ref>: C<(user_id =E<gt>  L<%user|"%user">,...)>

=back

=head2 C<bool> _extract_friends( I<$result_hash_ref, $friends_page_html_str> )

=over

=item * I<$result_hash_ref>: C<(user_id =E<gt> L<%user|"%user">,...)>

=back

=head2 C<bool> _extract_revs( I<$result_hash_ref, $on_progress_fn, $since_time_piece, $reviews_xhr_html_str> )

=over

=item * I<$result_hash_ref>: C<(review_id =E<gt> L<%review|"%review">,...)> 

=back

=head2 C<(L<%user|"%user">,...)> _extract_similar_authors( I<$author_id_to_skip, $similar_page_html_str> )

=head2 C<int> _check_page( I<$url, $html> )

=over

=item * returns 0 ok, 1 warn (ignore), 2 error (retry)

=item * warns if sign-in page (https://www.goodreads.com/user/sign_in) or in-page message

=item * warns if "page unavailable, Goodreads request took too long"

=item * warns if "page not found" 

=item * error if page unavailable: "An unexpected error occurred. 
        We will investigate this problem as soon as possible â€” please 
        check back soon!"

=item * error if over capacity (TODO UNTESTED):
        "<?>Goodreads is over capacity.</?> 
        <?>You can never have too many books, but Goodreads can sometimes
        have too many visitors. Don't worry! We are working to increase 
        our capacity.</?>
        <?>Please reload the page to try again.</?>
        <a ...>get the latest on Twitter</a>"
        https://pbs.twimg.com/media/DejvR6dUwAActHc.jpg
        https://pbs.twimg.com/media/CwMBEJAUIAA2bln.jpg
        https://pbs.twimg.com/media/CFOw6YGWgAA1H9G.png  (with title)

=item * error if maintenance mode (TODO UNTESTED):
        "<?>Goodreads is down for maintenance.</?>
        <?>We expect to be back within minutes. Please try again soon!<?>
        <a ...>Get the latest on Twitter</a>"
        https://pbs.twimg.com/media/DgKMR6qXUAAIBMm.jpg
        https://i.redditmedia.com/-Fv-2QQx2DeXRzFBRKmTof7pwP0ZddmEzpRnQU1p9YI.png

=item * error if website temporarily unavailable (TODO UNTESTED):
        "Our website is currently unavailable while we make some improvements
        to our service. We'll be open for business again soon,
        please come back shortly to try again. <?>
        Thank you for your patience." (No Alice error)
        https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/hostedimages/1404319071i/10224522.png

=back

=head2 C<string> _html( I<$url> )

=over

=item * HTML body of a web document

=item * might stop process on severe problems

=back

